{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 412,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: scikit-learn in /home/decker/.local/lib/python3.10/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/decker/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/lib/python3/dist-packages (from scikit-learn) (1.8.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/decker/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/decker/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: gensim in /home/decker/.local/lib/python3.10/site-packages (4.3.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/decker/.local/lib/python3.10/site-packages (from gensim) (1.24.3)\r\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/lib/python3/dist-packages (from gensim) (1.8.0)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/decker/.local/lib/python3.10/site-packages (from gensim) (6.4.0)\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting seqeval\r\n",
      "  Using cached seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/decker/.local/lib/python3.10/site-packages (from seqeval) (1.24.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/decker/.local/lib/python3.10/site-packages (from seqeval) (1.4.0)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/lib/python3/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.8.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/decker/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/decker/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install gensim\n",
    "!pip install seqeval"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:57:47.458208815Z",
     "start_time": "2024-02-15T03:57:41.965876181Z"
    }
   },
   "id": "e3bc4adee59adbd6"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: scikit-learn in /home/decker/.local/lib/python3.10/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/decker/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/lib/python3/dist-packages (from scikit-learn) (1.8.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/decker/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/decker/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: gensim in /home/decker/.local/lib/python3.10/site-packages (4.3.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/decker/.local/lib/python3.10/site-packages (from gensim) (1.24.3)\r\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/lib/python3/dist-packages (from gensim) (1.8.0)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/decker/.local/lib/python3.10/site-packages (from gensim) (6.4.0)\r\n",
      "--2024-02-13 21:23:59--  http://vectors.nlpl.eu/repository/20/6.zip\r\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\r\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 635351287 (606M) [application/zip]\r\n",
      "Saving to: ‘6.zip’\r\n",
      "\r\n",
      "6.zip               100%[===================>] 605.92M  6.10MB/s    in 2m 17s  \r\n",
      "\r\n",
      "2024-02-13 21:26:18 (4.44 MB/s) - ‘6.zip’ saved [635351287/635351287]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!wget http://vectors.nlpl.eu/repository/20/6.zip"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-14T05:23:56.782663649Z"
    }
   },
   "id": "8528bbea2bb73dfa"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./word2vec/6.zip\r\n",
      "  inflating: meta.json               \r\n",
      "  inflating: model.bin               \r\n",
      "  inflating: model.txt               \r\n",
      "  inflating: README                  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip ./word2vec/6"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T05:31:48.208015632Z",
     "start_time": "2024-02-14T05:31:39.243100924Z"
    }
   },
   "id": "14cd8fb9e41f69f8"
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, Tensor, optim\n",
    "import torch.nn.functional as F # Functions module - activations, utilities like padding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.scheme import IOB2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:57:57.797241747Z",
     "start_time": "2024-02-15T03:57:57.784073039Z"
    }
   },
   "id": "b3e5487bfc0e6607"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T06:45:01.212102706Z",
     "start_time": "2024-02-14T06:45:01.167911145Z"
    }
   },
   "id": "8747ff690f181544"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actor.gender': 0, 'gr.amount': 1, 'movie.country': 2, 'movie.directed_by': 3, 'movie.estimated_budget': 4, 'movie.genre': 5, 'movie.gross_revenue': 6, 'movie.initial_release_date': 7, 'movie.language': 8, 'movie.locations': 9, 'movie.music': 10, 'movie.produced_by': 11, 'movie.production_companies': 12, 'movie.rating': 13, 'movie.starring.actor': 14, 'movie.starring.character': 15, 'movie.subjects': 16, 'none': 17, 'person.date_of_birth': 18}\n",
      "{0: 'actor.gender', 1: 'gr.amount', 2: 'movie.country', 3: 'movie.directed_by', 4: 'movie.estimated_budget', 5: 'movie.genre', 6: 'movie.gross_revenue', 7: 'movie.initial_release_date', 8: 'movie.language', 9: 'movie.locations', 10: 'movie.music', 11: 'movie.produced_by', 12: 'movie.production_companies', 13: 'movie.rating', 14: 'movie.starring.actor', 15: 'movie.starring.character', 16: 'movie.subjects', 17: 'none', 18: 'person.date_of_birth'}\n"
     ]
    }
   ],
   "source": [
    "def create_item_list(df):\n",
    "    # Convert to list of dict's, \n",
    "    item_list = [row.to_dict() for idx, row in df.iterrows()]\n",
    "    \n",
    "    # Shuffle \n",
    "    np.random.shuffle(item_list)\n",
    "    \n",
    "    # Ensure utt's and iob tags are matched\n",
    "    item_list = [item for item in item_list if len(item['utterances'].split(' ')) == len(item['IOB Slot tags'])]\n",
    "    return item_list\n",
    "    \n",
    "    \n",
    "def get_relations(item_list):\n",
    "    relation_to_idx = list(sorted(set([j for i in item_list for j in i[\"Core Relations\"]])))\n",
    "    relation_to_idx = {r:i for i, r in enumerate(relation_to_idx)}\n",
    "    idx_to_relation = {i:r for i, r in enumerate(relation_to_idx)}\n",
    "    print(relation_to_idx)\n",
    "    print(idx_to_relation)\n",
    "    return relation_to_idx, idx_to_relation\n",
    "    \n",
    "\n",
    "# Load train csv\n",
    "train_csv_df = pd.read_csv(\"hw1_train.csv\")\n",
    "train_csv_df['IOB Slot tags'] = train_csv_df['IOB Slot tags'].replace(np.nan, \"none\")\n",
    "train_csv_df['Core Relations'] = train_csv_df['Core Relations'].replace(np.nan, \"none\")\n",
    "\n",
    "# Tokenize train\n",
    "train_csv_df[\"IOB Slot tags\"] = train_csv_df[\"IOB Slot tags\"].apply(lambda tags: tags.split())\n",
    "train_csv_df[\"Core Relations\"] = train_csv_df[\"Core Relations\"].apply(lambda tags: tags.split())\n",
    "\n",
    "item_list = create_item_list(train_csv_df)\n",
    "relation_to_idx, idx_to_relation = get_relations(item_list)\n",
    "\n",
    "# Load test csv\n",
    "test_csv_df = pd.read_csv(\"hw1_test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T06:14:44.721222571Z",
     "start_time": "2024-02-14T06:14:44.559752984Z"
    }
   },
   "id": "a282218ff186072a"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# Split train csv into train and val\n",
    "train_df, val_df = train_test_split(item_list, test_size=0.1, train_size=0.9)\n",
    "\n",
    "# Convert to lists\n",
    "train_data = [(i['utterances'], i['IOB Slot tags'], i['Core Relations']) for i in train_df]\n",
    "val_data = [(i['utterances'], i['IOB Slot tags'], i['Core Relations']) for i in val_df]\n",
    "\n",
    "train_texts = [p[0] for p in train_data]\n",
    "train_bios = [p[1] for p in train_data]\n",
    "train_rels = [p[2] for p in train_data]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T06:14:44.890063938Z",
     "start_time": "2024-02-14T06:14:44.871236448Z"
    }
   },
   "id": "12d6d8a567e33c4c"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# Load word2vec weights\n",
    "word2vec_weights = gensim.models.KeyedVectors.load_word2vec_format('word2vec/model.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T06:15:24.940925640Z",
     "start_time": "2024-02-14T06:14:45.016468891Z"
    }
   },
   "id": "6a4f67ce55edf2e1"
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "outputs": [],
   "source": [
    "class VocabularyEmbedding(object):\n",
    "    # For representing vocab embeddings\n",
    "    def __init__(self, gensim_w2v):\n",
    "\n",
    "        self.w2v = gensim_w2v\n",
    "        self.w2v.add_vector('<s>', np.random.uniform(low=-1, high=1.0, size=(300,)))\n",
    "        self.w2v.add_vector('</s>', np.random.uniform(low=-1, high=1.0, size=(300,)))\n",
    "        self.w2v.add_vector('<pad>', np.random.uniform(low=-1, high=1.0, size=(300,)))\n",
    "        self.w2v.add_vector('<unk>', np.random.uniform(low=-1, high=1.0, size=(300,)))\n",
    "\n",
    "        bos = self.w2v.key_to_index.get('<s>')\n",
    "        eos = self.w2v.key_to_index.get('</s>')\n",
    "        pad = self.w2v.key_to_index.get('<pad>')\n",
    "        unk = self.w2v.key_to_index.get('<unk>')\n",
    "\n",
    "        self.bos_index = bos\n",
    "        self.eos_index = eos\n",
    "        self.pad_index = pad\n",
    "        self.unk_index = unk\n",
    "\n",
    "    def tokenizer(self, text):\n",
    "        return [t for t in text.split(' ')]\n",
    "\n",
    "    def encode(self, text):\n",
    "\n",
    "        sequence = []\n",
    "\n",
    "        tokens = self.tokenizer(text)\n",
    "        for token in tokens:\n",
    "\n",
    "            index = self.w2v.key_to_index.get(token, self.unk_index)\n",
    "            sequence.append(index)\n",
    "\n",
    "        return sequence\n",
    "\n",
    "    def create_padded_tensor(self, sequences):\n",
    "        # sequences:\n",
    "        #print(sequences)\n",
    "\n",
    "        lengths = [len(sequence) for sequence in sequences]\n",
    "        max_seq_len = max(lengths)\n",
    "        tensor = torch.full((len(sequences), max_seq_len), self.pad_index, dtype=torch.long)\n",
    "\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            for j, token in enumerate(sequence):\n",
    "                tensor[i][j] = token\n",
    "\n",
    "        return tensor, lengths\n",
    "\n",
    "\n",
    "class BIOTagSequencer(object):\n",
    "    # For representing BIO tags\n",
    "    def __init__(self, tag_corpus, bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>'):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.unk_index = self.add_token(unk_token)\n",
    "        self.pad_index = self.add_token(pad_token)\n",
    "        self.bos_index = self.add_token(bos_token)\n",
    "        self.eos_index = self.add_token(eos_token)\n",
    "        self.tokenizer = lambda text: [t for t in text]\n",
    "\n",
    "        for _tags in tag_corpus:\n",
    "          for _token in self.tokenizer(_tags):\n",
    "            self.add_token(_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.word2idx:\n",
    "          self.word2idx[token] = new_index = len(self.word2idx)\n",
    "          self.idx2word[new_index] = token\n",
    "          return new_index\n",
    "\n",
    "        else:\n",
    "          return self.word2idx[token]\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenizer(text)\n",
    "\n",
    "        sequence = []\n",
    "\n",
    "        for token in tokens:\n",
    "\n",
    "            index = self.word2idx.get(token, self.unk_index)\n",
    "            sequence.append(index)\n",
    "\n",
    "        return sequence\n",
    "\n",
    "    def create_padded_tensor(self, sequences):\n",
    "\n",
    "        lengths = [len(sequence) for sequence in sequences]\n",
    "        max_seq_len = max(lengths)\n",
    "        tensor = torch.full((len(sequences), max_seq_len), self.pad_index, dtype=torch.long)\n",
    "\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            for j, token in enumerate(sequence):\n",
    "                tensor[i][j] = token\n",
    "\n",
    "        return tensor, lengths\n",
    "    \n",
    "    \n",
    "class RelationSequencer:\n",
    "    # For representing relations\n",
    "    def __init__(self, relations):\n",
    "        pass\n",
    "\n",
    "    def encode(self, text):\n",
    "        return text\n",
    "    \n",
    "    def create_padded_tensor(self, sequences):\n",
    "        tensor = torch.full(size=(len(sequences), len(relation_to_idx)), fill_value=0, dtype=torch.float)\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            for token in sequence:\n",
    "                tensor[i][relation_to_idx[token]] = 1\n",
    "        return tensor\n",
    "        \n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Copied directly from PyTorch docs\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:10:06.224221428Z",
     "start_time": "2024-02-15T03:10:06.175105833Z"
    }
   },
   "id": "791f463b2c34e2ea"
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "outputs": [],
   "source": [
    "class BIORelDataset(Dataset):\n",
    "    def __init__(self, data, text_sequencer, bio_sequencer, rel_sequencer):\n",
    "        self.data = data\n",
    "        self.input_sequencer = text_sequencer\n",
    "        self.bio_sequencer = bio_sequencer\n",
    "        self.rel_sequencer = rel_sequencer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text, tags, relations = self.data[index]\n",
    "        x = self.input_sequencer.encode(text)\n",
    "        y_bio = self.bio_sequencer.encode(tags)\n",
    "        y_rel = self.rel_sequencer.encode(relations)\n",
    "        return x, y_bio, y_rel\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class BIORelationTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens, dim_model, dropout_p, num_heads, num_encoders, num_bio_tags, num_rel_tags):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Info\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "        \n",
    "        # Layers\n",
    "        self.positional_encoder = PositionalEncoding(d_model=dim_model, dropout=dropout_p)\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        \n",
    "        #encoder_layer = nn.TransformerEncoderLayer(dim_model, nhead=num_heads)\n",
    "        #self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoders,\n",
    "            num_decoder_layers=num_encoders,  # TODO\n",
    "            dropout=dropout_p\n",
    "        )\n",
    "        \n",
    "        self.out_rel = nn.Linear(dim_model, num_rel_tags)\n",
    "        self.out_bio = nn.Linear(dim_model, num_bio_tags)\n",
    "        \n",
    "    def forward(self, x, tgt_key):\n",
    "        # Input shape: (batch size, seq len)\n",
    "        x = self.embedding(x) * math.sqrt(self.dim_model)\n",
    "        x = self.positional_encoder(x)\n",
    "        \n",
    "        # Reshape to (seq len, batch size, dim model)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        #t_out = self.transformer_encoder(x)\n",
    "        # mask = self.transformer.generate_square_subsequent_mask(len(x))  # TODO: create padding mask\n",
    "        t_out = self.transformer(x, x)\n",
    "        \n",
    "        # print(f'transformer out shape: {t_out.shape}')\n",
    "        \n",
    "        out_iob = self.out_bio(t_out)\n",
    "        out_rel = self.out_rel(torch.max(t_out, dim=0).values)\n",
    "        \n",
    "        # print(f'out_iob shape: {out_iob.shape}')\n",
    "        # print(f'out_rel shape: {out_rel.shape}')\n",
    "        \n",
    "        return out_iob, out_rel\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:10:06.389762989Z",
     "start_time": "2024-02-15T03:10:06.371066954Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/decker/.local/lib/python3.10/site-packages/gensim/models/keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_sequencer = VocabularyEmbedding(word2vec_weights)\n",
    "bio_sequencer = BIOTagSequencer(train_bios)\n",
    "rel_sequencer = RelationSequencer(train_rels)\n",
    "\n",
    "train_dataset = BIORelDataset(train_data, text_sequencer, bio_sequencer, rel_sequencer)\n",
    "val_dataset = BIORelDataset(val_data, text_sequencer, bio_sequencer, rel_sequencer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:13:07.841537747Z",
     "start_time": "2024-02-15T03:13:07.009227037Z"
    }
   },
   "id": "191ad28f27853094"
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "outputs": [],
   "source": [
    "def prepare_batch(batch, in_sequencer, bio_sequencer, rel_sequencer):\n",
    "    texts, bio_tags, rel_tags = zip(*batch)\n",
    "    text_tensor, lengths = in_sequencer.create_padded_tensor(texts)\n",
    "    bio_tensor, _ = bio_sequencer.create_padded_tensor(bio_tags)\n",
    "    rel_tensor = rel_sequencer.create_padded_tensor(rel_tags)\n",
    "    return text_tensor, lengths, bio_tensor, rel_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:13:07.841905855Z",
     "start_time": "2024-02-15T03:13:07.834281325Z"
    }
   },
   "id": "d37f7a8f1f720651"
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=lambda batch: prepare_batch(batch, text_sequencer, bio_sequencer, rel_sequencer))\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=lambda batch: prepare_batch(batch, text_sequencer, bio_sequencer, rel_sequencer), shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:13:07.842050549Z",
     "start_time": "2024-02-15T03:13:07.837242655Z"
    }
   },
   "id": "fe2b1c8732c98454"
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/decker/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "model = BIORelationTransformer(num_tokens=len(text_sequencer.w2v),\n",
    "                               dim_model=64, \n",
    "                               num_heads=2,\n",
    "                               num_encoders=4,\n",
    "                               dropout_p=0.1,\n",
    "                               num_bio_tags=len(bio_sequencer.word2idx),\n",
    "                               num_rel_tags=len(relation_to_idx)).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn_bio = nn.CrossEntropyLoss(ignore_index=bio_sequencer.pad_index)\n",
    "loss_fn_rel = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:13:08.013355703Z",
     "start_time": "2024-02-15T03:13:07.843501427Z"
    }
   },
   "id": "faf21e96908f4710"
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss_fn_bio, loss_fn_rel, dataloader):\n",
    "    # Help from https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        X = batch[0].to(device)\n",
    "        y_bio = batch[2].to(device)\n",
    "        y_rel = batch[3].to(device)\n",
    "        \n",
    "        pred_bio, pred_rel = model(X)\n",
    "        \n",
    "        # Permute bio to (batch size, seq len, labels)\n",
    "        pred_bio = pred_bio.permute(1, 2, 0)\n",
    "        \n",
    "        # print(pred_bio[0][0])\n",
    "        # print(f'y_bio shape: {y_bio.shape}')\n",
    "        # print(f'pred_bio shape: {pred_bio.shape}')\n",
    "        # print(f'\\ny_rel shape: {y_rel.shape}')\n",
    "        # print(f'pred_rel shape: {pred_rel.shape}')\n",
    "        \n",
    "        loss = loss_fn_rel(pred_rel, y_rel) + loss_fn_bio(pred_bio, y_bio)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss += loss.detach().item()\n",
    "        print(f'{i} / {len(dataloader)}: {loss}')\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:16:55.241840457Z",
     "start_time": "2024-02-15T03:16:55.195993520Z"
    }
   },
   "id": "17b29efb61880589"
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 64: 5.8518757820129395\n",
      "1 / 64: 5.652836322784424\n",
      "2 / 64: 5.775554180145264\n",
      "3 / 64: 5.132939338684082\n",
      "4 / 64: 5.4045233726501465\n",
      "5 / 64: 4.924869537353516\n",
      "6 / 64: 4.258816242218018\n",
      "7 / 64: 4.620996952056885\n",
      "8 / 64: 4.089197158813477\n",
      "9 / 64: 4.168509483337402\n",
      "10 / 64: 4.163624286651611\n",
      "11 / 64: 4.101083755493164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_25773/1119581379.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain_loop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_fn_bio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_fn_rel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# batch size: 32\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# model dim:  64\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# seq len:    13\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_25773/2025478581.py\u001B[0m in \u001B[0;36mtrain_loop\u001B[0;34m(model, opt, loss_fn_bio, loss_fn_rel, dataloader)\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0my_rel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m         \u001B[0mpred_bio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpred_rel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0;31m# Permute bio to (batch size, seq len, labels)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1510\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1511\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1512\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1513\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1518\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1519\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1521\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1522\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_25773/3119665825.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     53\u001B[0m         \u001B[0;31m# Transformer blocks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m         \u001B[0;31m#t_out = self.transformer_encoder(x)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 55\u001B[0;31m         \u001B[0mt_out\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransformer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m         \u001B[0;31m# print(f'transformer out shape: {t_out.shape}')\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1510\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1511\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1512\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1513\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1518\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1519\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1521\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1522\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[1;32m    204\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"the feature number of src and tgt must be equal to d_model\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 206\u001B[0;31m         memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask,\n\u001B[0m\u001B[1;32m    207\u001B[0m                               is_causal=src_is_causal)\n\u001B[1;32m    208\u001B[0m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1510\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1511\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1512\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1513\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1518\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1519\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1521\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1522\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001B[0m\n\u001B[1;32m    389\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    390\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmod\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 391\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_causal\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mis_causal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_key_padding_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msrc_key_padding_mask_for_layers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    392\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    393\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mconvert_to_nested\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1510\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1511\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1512\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1513\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1518\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1519\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1521\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1522\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001B[0m\n\u001B[1;32m    712\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ff_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    713\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 714\u001B[0;31m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sa_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc_key_padding_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_causal\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mis_causal\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    715\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ff_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    716\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1505\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1506\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1507\u001B[0;31m     \u001B[0;32mdef\u001B[0m \u001B[0m_wrapped_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1508\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1509\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(model, opt, loss_fn_bio, loss_fn_rel, train_loader)\n",
    "\n",
    "# batch size: 32\n",
    "# model dim:  64\n",
    "# seq len:    13\n",
    "# num bio labels: 30\n",
    "# num rel labels: 19\n",
    "#print(len(rel_sequencer.word2idx))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:16:59.143183384Z",
     "start_time": "2024-02-15T03:16:55.450083487Z"
    }
   },
   "id": "9deb61231b58dd25"
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "outputs": [],
   "source": [
    "def val_loop(model, loss_fn_bio, loss_fn_rel, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    bio_targets = []\n",
    "    predicted_bio_labels = []\n",
    "    rel_targets = []\n",
    "    predicted_rel_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch[0].to(device)\n",
    "            y_bio = batch[2].to(device)\n",
    "            y_rel = batch[3].to(device)\n",
    "\n",
    "            logits_bio, logits_rel = model(X)\n",
    "\n",
    "            # Permute bio to (batch size, seq len, labels)\n",
    "            logits_bio = logits_bio.permute(1, 2, 0)\n",
    "            \n",
    "            loss = loss_fn_rel(logits_rel, y_rel) + loss_fn_bio(logits_bio, y_bio)\n",
    "            total_loss += loss.detach().item()\n",
    "            \n",
    "            # Softmax and argmax over possible labels\n",
    "            probs_bio = F.softmax(logits_bio, dim=-2)\n",
    "            preds_bio = torch.argmax(probs_bio, dim=-2)\n",
    "            probs_rel = F.softmax(logits_rel, dim=-2)\n",
    "            preds_rel = torch.argmax(probs_rel, dim=-2)\n",
    "            \n",
    "            bio_targets.append(y_bio.tolist())\n",
    "            rel_targets.append(y_rel.tolist())\n",
    "            predicted_bio_labels.append(preds_bio.tolist())\n",
    "            predicted_rel_labels.append(preds_rel.tolist())\n",
    "            \n",
    "    # Get bio targets and predictions without the padding\n",
    "    non_padding_bio_targets = [[_x[i] for i in range(len(_x)) if _x[i] != bio_sequencer.pad_index] for _batch in bio_targets for _x in _batch]\n",
    "    non_padding_bio_predictions = [[_x[i] for i in range(len(_x)) if _x[i] != bio_sequencer.pad_index] for _batch in predicted_bio_labels for _x in _batch]\n",
    "    \n",
    "    # TODO: hacky, get padding working with model\n",
    "    non_padding_bio_predictions = [[preds[i] for i in range(len(targets))] for preds, targets in zip(non_padding_bio_predictions, non_padding_bio_targets)]\n",
    "    \n",
    "    #[print(t, p) for t, p in zip(non_padding_bio_targets, non_padding_bio_predictions)]\n",
    "    \n",
    "    # Convert from one hot to text labels\n",
    "    bio_true_labels = [[str(bio_sequencer.idx2word[_y]).replace('_', '-') for _y in _x] for _x in non_padding_bio_targets]\n",
    "    bio_predicted_labels = [[str(bio_sequencer.idx2word[_y]).replace('_', '-') for _y in _x][:len(_t)] for _x, _t in zip(non_padding_bio_predictions, bio_true_labels)]\n",
    "    \n",
    "    #[print(t, p) for t, p in zip(bio_true_labels, bio_predicted_labels)]\n",
    "    \n",
    "    print(classification_report(bio_true_labels, bio_predicted_labels, scheme=IOB2))\n",
    "    bio_f1 = f1_score(bio_true_labels, bio_predicted_labels, scheme=IOB2)\n",
    "    #print(bio_f1)\n",
    "    \n",
    "    # TODO: get f1 for relations\n",
    "    \n",
    "    return total_loss / len(dataloader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T04:12:32.761630305Z",
     "start_time": "2024-02-15T04:12:32.738507833Z"
    }
   },
   "id": "df8ee5ff4f1b48ca"
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cast       0.00      0.00      0.00        11\n",
      "     country       0.00      0.00      0.00        23\n",
      "    director       0.00      0.00      0.00        21\n",
      "       genre       0.00      0.00      0.00         3\n",
      "    language       0.00      0.00      0.00         6\n",
      "       movie       0.00      0.00      0.00        95\n",
      " mpaa-rating       0.00      0.00      0.00        12\n",
      "      person       0.00      0.00      0.00        22\n",
      "    producer       0.00      0.00      0.00        18\n",
      "release-year       0.00      0.00      0.00         1\n",
      "     subject       0.00      0.00      0.00         8\n",
      "        unk>       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       221\n",
      "   macro avg       0.00      0.00      0.00       221\n",
      "weighted avg       0.00      0.00      0.00       221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/decker/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <unk> seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/decker/.local/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/decker/.local/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": "4.272278845310211"
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loop(model, loss_fn_bio, loss_fn_rel, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T04:12:33.441027530Z",
     "start_time": "2024-02-15T04:12:33.217239216Z"
    }
   },
   "id": "27ca27ab71d73cbd"
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<unk>', 1: '<pad>', 2: '<s>', 3: '</s>', 4: 'O', 5: 'B_subject', 6: 'B_producer', 7: 'B_person', 8: 'I_person', 9: 'B_movie', 10: 'I_movie', 11: 'I_subject', 12: 'B_genre', 13: 'B_language', 14: 'B_mpaa_rating', 15: 'B_cast', 16: 'I_cast', 17: 'I_producer', 18: 'B_director', 19: 'I_director', 20: 'B_country', 21: 'B_char', 22: 'I_country', 23: 'I_language', 24: 'I_char', 25: 'B_release_year', 26: 'I_genre', 27: 'I_mpaa_rating', 28: 'B_location', 29: 'I-movie'}\n"
     ]
    }
   ],
   "source": [
    "print(bio_sequencer.idx2word)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T03:29:17.489615908Z",
     "start_time": "2024-02-15T03:29:17.475490578Z"
    }
   },
   "id": "c032c0fa62dce8eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, opt, loss_fn, train_loader, val_loader, epochs):\n",
    "    train_loss_list, val_loss_list = [], []\n",
    "    \n",
    "    for epoch, range(epochs):\n",
    "        print(f'-----Epoch {epoch}')\n",
    "        train_loss = train_loop(model, opt, loss_fn, train_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        \n",
    "        # TODO: val\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T01:58:50.831222771Z",
     "start_time": "2024-02-15T01:58:50.830470567Z"
    }
   },
   "id": "8a51b9ece46afd75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b135b8aab5278f12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
